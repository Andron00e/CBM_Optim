{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12d2cae2-1771-495d-ba15-f58242a3b059",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import requests\n",
    "import numpy as np\n",
    "import transformers\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import skimage.io as io\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45e131bc-cc37-4d37-9bfa-bac4361cba8b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8 GPU(s) available.\n",
      "We will use the GPU: NVIDIA A100-SXM4-80GB\n"
     ]
    }
   ],
   "source": [
    "import configs\n",
    "\n",
    "configs.set_seed(42)\n",
    "device = configs.set_device(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2145feea-2523-494d-9e00-5fdabf9bcc74",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-01 14:13:19.227323: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-01 14:13:19.227408: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-01 14:13:19.227429: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-01 14:13:19.234420: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-01 14:13:20.101417: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "model = transformers.CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = transformers.CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef50fbd-f7d3-4a49-aef2-b327471a42fb",
   "metadata": {},
   "source": [
    "## data utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e372312f-f4f7-4e8a-9ab6-1c51777e6f24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from configs import *\n",
    "import transformers\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'image': [x['image'] for x in batch],\n",
    "        'labels': [x['labels'] for x in batch]\n",
    "    }\n",
    "\n",
    "def remove_prefixes(strings):\n",
    "    prefixes = ['a', 'an', 'the']\n",
    "    result = []\n",
    "\n",
    "    for string in strings:\n",
    "        words = string.split()\n",
    "        if words[0].lower() in prefixes:\n",
    "            result.append(' '.join(words[1:]))\n",
    "        else:\n",
    "            result.append(string)\n",
    "\n",
    "    return result\n",
    "\n",
    "def preprocess_loader(loader, concepts: list):\n",
    "    preprocessed_batches = []\n",
    "    processor = transformers.CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    for batch in tqdm(loader):\n",
    "        preprocessed_batch = preprocess_batch(batch, processor, concepts)\n",
    "        preprocessed_batches.append(preprocessed_batch)\n",
    "    return preprocessed_batches\n",
    "\n",
    "def preprocess_batch(batch, processor, concepts: list):\n",
    "    return processor(text=concepts, images=batch['image'], return_tensors=\"pt\", padding=True), batch['labels']\n",
    "\n",
    "def prepared_dataloaders(hf_link: str, concepts: list, test_size: int=0.2, prep_loaders='all'):\n",
    "    from datasets import load_dataset\n",
    "    from datasets import DatasetDict\n",
    "    dataset = load_dataset(hf_link)\n",
    "    dataset = dataset[\"train\"].train_test_split(test_size=0.2)\n",
    "    val_test = dataset[\"test\"].train_test_split(test_size=0.5)\n",
    "    dataset = DatasetDict({\n",
    "        \"train\": dataset[\"train\"],\n",
    "        \"validation\": val_test[\"train\"],\n",
    "        \"test\": val_test[\"test\"],\n",
    "    })\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset[\"train\"], batch_size=32, shuffle=True, collate_fn=collate_fn, pin_memory=True)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset[\"validation\"], batch_size=32, shuffle=False, collate_fn=collate_fn, pin_memory=True)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset[\"test\"], batch_size=32, shuffle=False, collate_fn=collate_fn, pin_memory=True)\n",
    "\n",
    "    if prep_loaders == 'all':\n",
    "        train_loader_preprocessed = preprocess_loader(train_loader, concepts)\n",
    "        val_loader_preprocessed = preprocess_loader(val_loader, concepts)\n",
    "        test_loader_preprocessed = preprocess_loader(test_loader, concepts)\n",
    "    elif prep_loaders == 'train':\n",
    "        train_loader_preprocessed = preprocess_loader(train_loader, concepts)\n",
    "        return train_loader_preprocessed\n",
    "    elif prep_loaders == 'val':\n",
    "        val_loader_preprocessed = preprocess_loader(val_loader, concepts)\n",
    "        return val_loader_preprocessed\n",
    "    elif prep_loaders == 'test':\n",
    "        test_loader_preprocessed = preprocess_loader(test_loader, concepts)\n",
    "        return test_loader_preprocessed\n",
    "\n",
    "    #return train_loader_preprocessed, val_loader_preprocessed, test_loader_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e01980a5-e11f-4f86-90e3-f71572abad43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"conceptnet_cifar10_filtered_new.txt\", \"r\") as f:\n",
    "    concepts = f.read().lower().split(\"\\n\")\n",
    "    concepts = remove_prefixes(concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a44edbb2-2095-4e7c-834f-6d30ad2458f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 188/188 [00:34<00:00,  5.47it/s]\n"
     ]
    }
   ],
   "source": [
    "val_loader_preprocessed = prepared_dataloaders(\"Andron00e/CIFAR10-custom\", \n",
    "                                                concepts=concepts,\n",
    "                                                prep_loaders=\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f952a3c-6a85-45f8-b942-c80a32d98609",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e7967690-43f4-4f51-960c-298741677c17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "classes = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d0f1e4b1-7b3d-45bf-9c39-01e61df574d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_class_features(model, classes:list, model_name: str=\"openai/clip-vit-base-patch32\", device=device):\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    inputs = tokenizer(classes, padding=True, return_tensors=\"pt\").to(device)\n",
    "    features = model.get_text_features(inputs['input_ids'])\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0d7db0de-5602-45c1-b810-c6a95071ac2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_features = get_class_features(model, classes, \"openai/clip-vit-large-patch14\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "681bda02-1320-4c33-bb2a-bf6fa35897fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "def CMS(loader, model, class_features, device=device): #hf version\n",
    "    accuracy_metric = load_metric(\"accuracy\")\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "        class_features = class_features.to(device)\n",
    "        clip_accuracy, cms_accuracy = [], []\n",
    "        for step, batch in enumerate(loader):\n",
    "            inputs, labels = batch\n",
    "            inputs, targets = inputs.to(device), torch.LongTensor(labels).to(device)\n",
    "            \n",
    "            image_features = model.get_image_features(inputs['pixel_values'])\n",
    "            concept_features = model.get_text_features(inputs['input_ids'])\n",
    "            \n",
    "            V_matrix = image_features @ concept_features.T\n",
    "            T_matrix = class_features @ concept_features.T\n",
    "            \n",
    "            image_class_similarity = image_features @ class_features.T\n",
    "            \n",
    "            clip_preds = image_class_similarity.argmax(dim=1)\n",
    "            clip_acc = accuracy_metric.compute(references=targets.cpu(), predictions=clip_preds.cpu())\n",
    "            \n",
    "            similarity = (V_matrix / V_matrix.norm(dim=1, keepdim=True)) @ (T_matrix / T_matrix.norm(dim=1, keepdim=True)).T\n",
    "            cms_preds = similarity.argmax(dim=1)\n",
    "            cms_acc = accuracy_metric.compute(references=targets.cpu(), predictions=cms_preds.cpu())\n",
    "            \n",
    "            clip_accuracy.append(clip_acc['accuracy'])\n",
    "            cms_accuracy.append(cms_acc['accuracy'])\n",
    "            \n",
    "    return np.mean(clip_accuracy), np.mean(cms_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "250a7eb3-161a-476c-bd5d-0fa30c5337ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP accuracy:  0.6032247340425532 \n",
      "\n",
      "CMS accuracy:  0.29188829787234044 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "clip_acc, cms_acc = CMS(val_loader_preprocessed, model, class_features, device)\n",
    "print(\"CLIP accuracy: \", clip_acc, \"\\n\")\n",
    "print(\"CMS accuracy: \", cms_acc, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f4a138-30bb-432c-a6bd-d8b837450dd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e687814c-e813-43ef-b38d-3054f2ba6212",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea72bc0b-7611-40d6-87af-39d5612a9684",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f37d5e0-f463-4c0d-aeba-4893b9235c07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab9d13f-d4dd-4033-9a5c-73ba2af3e1e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c237908-774d-48ce-a86c-083b004bff08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "andronserv",
   "language": "python",
   "name": "andronserv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
