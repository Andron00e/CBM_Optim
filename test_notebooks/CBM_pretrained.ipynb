{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = True\n",
    "image_path = \"\" # to set\n",
    "captions_path = \"\"# to set\n",
    "batch_size = 8\n",
    "num_workers = 0\n",
    "lr = 1e-3\n",
    "weight_decay = 1e-3\n",
    "patience = 2\n",
    "factor = 0.5\n",
    "epochs = 5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = 'resnet50'\n",
    "image_embedding = 2048\n",
    "text_encoder_model = \"distilbert-base-uncased\"\n",
    "text_embedding = 768\n",
    "text_tokenizer = \"distilbert-base-uncased\"\n",
    "max_length = 200\n",
    "\n",
    "pretrained = False # for both image encoder and text encoder\n",
    "trainable = False # for both image encoder and text encoder\n",
    "temperature = 1.0\n",
    "\n",
    "# image size\n",
    "size = 224\n",
    "\n",
    "# for projection head; used for both image and text encoders\n",
    "num_projection_layers = 1\n",
    "projection_dim = 256 \n",
    "dropout = 0.1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertModel, DistilBertConfig\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, model_name=text_encoder_model, pretrained=pretrained, trainable=trainable):\n",
    "        super().__init__()\n",
    "        if pretrained:\n",
    "            self.model = DistilBertModel.from_pretrained(model_name)\n",
    "        else:\n",
    "            self.model = DistilBertModel(config=DistilBertConfig())\n",
    "\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = trainable # False\n",
    "\n",
    "        self.target_token_idx = 0\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = output.last_hidden_state\n",
    "        return last_hidden_state[:, self.target_token_idx, :]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "projection head implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuickGELU(nn.Module): # instead of GELU\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return x * torch.sigmoid(1.702 * x)\n",
    "        \n",
    "\n",
    "class QuickLayerNorm(nn.LayerNorm): # instead of LayerNorm\n",
    "    '''for fp16'''\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        original_type = x.dtype\n",
    "        ret = super().forward(x.type(torch.float32))\n",
    "        return ret.type(original_type)\n",
    "    \n",
    "\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, embedding_dim, projection_dim=projection_dim, dropout=dropout):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(embedding_dim, projection_dim)\n",
    "        self.gelu = QuickGELU() #nn.GELU()\n",
    "        self.fc = nn.Linear(projection_dim, projection_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = QuickLayerNorm(projection_dim)#nn.LayerNorm(projection_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        projected = self.projection(x)\n",
    "        x = self.gelu(projected)\n",
    "        x = self.fc(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + projected\n",
    "        x = self.layer_norm(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ImageEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, model_name=model_name, pretrained=pretrained, trainable=trainable):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_name, pretrained, num_classes=0, global_pool=\"avg\")\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = trainable # False\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2048])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im_m = ImageEncoder()\n",
    "x  = torch.randn(1, 3, 224, 224)\n",
    "im_m(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 25, 768])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_m = TextEncoder()\n",
    "y_ids = torch.randint(5, 300, size=(8, 25))\n",
    "attention_mask = torch.ones(8, 25)\n",
    "text_m(y_ids, attention_mask).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ClassificationLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConceptClassifier(nn.Module):\n",
    "    def __init__(self,   embedding_dim, projection_dim, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.text_encoder_model = DistilBertModel.from_pretrained(text_encoder_model)\n",
    "        self.projection = nn.Linear(embedding_dim, projection_dim)\n",
    "        self.classifier_layer = nn.Linear(projection_dim, num_classes) # , sparse=True\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        hidden_states = self.text_encoder_model(input_ids=input_ids, attention_mask=attention_mask)[0]\n",
    "        pooled_output = hidden_states[:, 0]\n",
    "        projection_output = self.projection(pooled_output)\n",
    "        logits = self.classifier_layer(projection_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10])\n"
     ]
    }
   ],
   "source": [
    "'''test'''\n",
    "cls_model = ConceptClassifier(768, 256, num_classes=10)\n",
    "input_ids = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "attention_mask = torch.tensor([[1, 1, 1], [1, 1, 0]])\n",
    "logits = cls_model(input_ids, attention_mask)\n",
    "print(logits.shape) # should output torch.Size([2, 10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialize our loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(preds, targets, reduction=\"none\"): # take care of loss normalization by ourselves\n",
    "    log_softmax = nn.LogSoftmax(dim=-1)\n",
    "    loss = (-targets * log_softmax(preds)).sum(1)\n",
    "    if reduction == \"none\":\n",
    "        return loss\n",
    "    elif reduction == \"mean\":\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIP_model(nn.Module):\n",
    "    def __init__(self, temperature=temperature, image_embedding=image_embedding, text_embedding=text_embedding):\n",
    "        super().__init__()\n",
    "        self.image_encoder = ImageEncoder()\n",
    "        self.text_encoder = TextEncoder()\n",
    "        self.image_projection = ProjectionHead(embedding_dim=image_embedding)\n",
    "        self.text_projection = ProjectionHead(embedding_dim=text_embedding)\n",
    "        self.temperature = temperature\n",
    "        #self.classify = ConceptClassifier() #\n",
    "\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07)) #\n",
    "\n",
    "    def forward(self, batch):\n",
    "        image_features = self.image_encoder(batch[\"image\"])\n",
    "        text_features = self.text_encoder(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])\n",
    "\n",
    "        image_embeddings = self.image_projection(image_features)\n",
    "        text_embeddings = self.text_projection(text_features)\n",
    "        # at this step normalization is not necessary because of layer norm in ProjectionHead\n",
    "        # this part will be like in a CLIP trustworthy\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits_per_image = logit_scale * (image_embeddings @ text_embeddings.T) / self.temperature\n",
    "        logits_per_text = logits_per_image.T\n",
    "        #images_similarity = image_embeddings @ image_embeddings.T\n",
    "        #texts_similarity = text_embeddings @ text_embeddings.T\n",
    "        #targets = \n",
    "        return logits_per_image, logits_per_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 2])\n"
     ]
    }
   ],
   "source": [
    "# можно пока его поробовать\n",
    "class ConceptClassifierTest(nn.Module):\n",
    "    def __init__(self, embedding_dim, projection_dim, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.text_encoder_model = DistilBertModel.from_pretrained(text_encoder_model)\n",
    "        self.projection = nn.Linear(embedding_dim, projection_dim)\n",
    "        self.classifier_layer = nn.Embedding(num_classes, projection_dim, sparse=True)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        hidden_states = self.text_encoder_model(input_ids=input_ids, attention_mask=attention_mask)[0]\n",
    "        pooled_output = hidden_states[:, 0]\n",
    "        projection_output = self.projection(pooled_output)\n",
    "        logits = self.classifier_layer.weight.matmul(projection_output.T)\n",
    "        return logits\n",
    "    \n",
    "'''test'''\n",
    "cls_model = ConceptClassifierTest(768, 256, num_classes=10)\n",
    "input_ids = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "attention_mask = torch.tensor([[1, 1, 1], [1, 1, 0]])\n",
    "logits = cls_model(input_ids, attention_mask)\n",
    "print(logits.shape) # should output torch.Size([2, 10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load():\n",
    "\n",
    "def get_target_model(target_name, device):\n",
    "    target_name = target_name[5:]\n",
    "    model, preprocess = clip.load(target_name, device=device)\n",
    "    target_model = lambda x: model.image_encoder(x).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/Trustworthy-ML-Lab/Label-free-CBM/blob/main/cbm.py#L37\n",
    "class CBM(nn.Module):\n",
    "    def __init__(self, backbone_name, W_c, W_g, b_g, proj_mean, proj_std, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        model, _ = get_target_model(backbone_name, device) # дописать\n",
    "        self.proj_layer = nn.Linear(in_features=W_c.shape[1], out_features=W_c.shape[0], bias=False).to(device)\n",
    "        self.proj_layer.load_state_dict({\"weight\":W_c})\n",
    "\n",
    "        self.proj_mean = proj_mean\n",
    "        self.proj_std = proj_std\n",
    "\n",
    "        self.final = nn.Linear(in_features=W_g.shape[1], out_features=W_g.shape[0], bias=False).to(device)\n",
    "        self.final.load_state_dict({\"weight\":W_g, \"bias\":b_g})\n",
    "        self.concepts = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.proj_layer(x)\n",
    "        proj_c = (x - self.proj_mean) / self.proj_std\n",
    "        x = self.final(proj_c)\n",
    "        return x, proj_c\n",
    "    # разобрать load_state_dict, разобрать и написать get_target model и load и как их реализовать в моем случае"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
