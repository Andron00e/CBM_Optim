{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPocqr+MYQm0tptIKZ4zqKc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Andron00e/CBM_Optim/blob/main/template_clip_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxle13PwVhS1"
      },
      "outputs": [],
      "source": [
        "! pip install open_clip_torch\n",
        "!pip install -U sentence-transformers\n",
        "!pip install openai-clip\n",
        "!pip install pytorchcv\n",
        "!pip install -q git+https://github.com/huggingface/peft.git git+https://github.com/huggingface/transformers.git\n",
        "!pip install accelerate\n",
        "!pip install bitsandbytes\n",
        "!pip install sentencepiece\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import open_clip\n",
        "from torchvision.datasets import CIFAR10\n",
        "from open_clip import tokenizer\n",
        "import torch\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
      ],
      "metadata": {
        "id": "pNRRLs-vV54Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "open_clip.list_pretrained()"
      ],
      "metadata": {
        "id": "yuZ0TIpIWIiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clip_model, _, preprocess = open_clip.create_model_and_transforms('NAME OF MODEL', pretrained='AUTHOR') # ViT-B-16 openai"
      ],
      "metadata": {
        "id": "_3wmg0SQV8CV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clip_model.eval()\n",
        "context_length = clip_model.context_length\n",
        "vocab_size = clip_model.vocab_size\n",
        "\n",
        "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in clip_model.parameters()]):,}\")\n",
        "print(\"Context length:\", context_length)\n",
        "print(\"Vocab size:\", vocab_size)"
      ],
      "metadata": {
        "id": "Hj_0QVoeWJH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cifar10 = CIFAR10(os.path.expanduser(\"~/.cache\"), transform=preprocess, download=True)"
      ],
      "metadata": {
        "id": "Lr000ze4WTzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_images = []\n",
        "images = []\n",
        "texts = []\n",
        "plt.figure(figsize=(16, 5))\n",
        "\n",
        "for filename in [filename for filename in os.listdir(\"/content/\") if filename.endswith(\".png\") or filename.endswith(\".jpg\")]:\n",
        "    image = Image.open(os.path.join(\"/content/\", filename)).convert(\"RGB\")\n",
        "\n",
        "    plt.subplot(2, 4, len(images) + 1)\n",
        "    plt.imshow(image)\n",
        "    plt.title(f\"{filename}\")\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "\n",
        "    images.append(preprocess(image))\n",
        "    original_images.append(image)\n",
        "\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "GAiXnAWtpfBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_descriptions = []\n",
        "\n",
        "with open(\"NAME_OF_THE_FILE_WITH_FILTERED_CONCEPTS\", \"r\") as file: #conceptnet_cifar10_filtered_new.txt\n",
        "    for line in file:\n",
        "        text_descriptions.append(line.strip())\n",
        "\n",
        "print(text_descriptions)"
      ],
      "metadata": {
        "id": "bz_oit62ppMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_input = torch.tensor(np.stack(images))\n",
        "\n",
        "with torch.no_grad():\n",
        "    image_features = clip_model.encode_image(image_input).float()\n",
        "\n",
        "image_features /= image_features.norm(dim=-1, keepdim=True)"
      ],
      "metadata": {
        "id": "C2V2IOfau09R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_tokens = tokenizer.tokenize(text_descriptions)\n",
        "\n",
        "with torch.no_grad():\n",
        "    text_features = clip_model.encode_text(text_tokens).float()\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "top_probs, top_labels = text_probs.cpu().topk(5, dim=-1)"
      ],
      "metadata": {
        "id": "_Q20uzc9p1w4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(16, 16))\n",
        "\n",
        "for i, image in enumerate(original_images):\n",
        "    plt.subplot(4, 4, 2 * i + 1)\n",
        "    plt.imshow(image)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(4, 4, 2 * i + 2)\n",
        "    y = np.arange(top_probs.shape[-1])\n",
        "    plt.grid()\n",
        "    plt.barh(y, top_probs[i])\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.gca().set_axisbelow(True)\n",
        "    plt.yticks(y, [text_descriptions[index] for index in top_labels[i].numpy()])\n",
        "    plt.xlabel(\"probability\")\n",
        "\n",
        "plt.subplots_adjust(wspace=0.5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tIaCeGqgp0MP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}