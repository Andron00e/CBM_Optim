{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Andron00e/CBM_Optim/blob/main/llama70Bprompts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "T3QW_fMV0Iwx",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "f550237f-3a9f-4871-b7fb-0085956c32fb"
   },
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/huggingface/peft.git git+https://github.com/huggingface/transformers.git\n",
    "# !pip install accelerate\n",
    "# !pip install bitsandbytes\n",
    "# !pip install sentencepiece\n",
    "# !pip install datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "NrdjhlZBoZIp"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "\n",
    "import auto_gptq\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('CUDA GPU available: ', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2GtDKXojpnSb"
   },
   "source": [
    "Llama-2-70B-chat-GPTQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpus = torch.cuda.device_count()\n",
    "\n",
    "# Iterate over the available GPUs\n",
    "for i in range(num_gpus):\n",
    "    # Get the properties of the GPU\n",
    "    properties = torch.cuda.get_device_properties(i)\n",
    "    \n",
    "    # Print the name and memory consumption of the GPU\n",
    "    print(f\"GPU {i}: {properties.name}, Memory: {properties.total_memory}MB\")\n",
    "device = torch.device(\"cuda:0\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "z5qOqWpEohD4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading model.safetensors: 100%|██████████| 40.7G/40.7G [15:09<00:00, 44.7MB/s]\n",
      "Downloading (…)neration_config.json: 100%|██████████| 137/137 [00:00<00:00, 294kB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 745/745 [00:00<00:00, 1.63MB/s]\n",
      "Downloading tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 154MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 3.31MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 411/411 [00:00<00:00, 913kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 8192, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-79): 80 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "          (k_proj): QuantLinear()\n",
      "          (o_proj): QuantLinear()\n",
      "          (q_proj): QuantLinear()\n",
      "          (v_proj): QuantLinear()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (act_fn): SiLUActivation()\n",
      "          (down_proj): QuantLinear()\n",
      "          (gate_proj): QuantLinear()\n",
      "          (up_proj): QuantLinear()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=8192, out_features=32000, bias=False)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_path = \"TheBloke/Llama-2-70B-chat-GPTQ\"\n",
    "# tutorial: https://huggingface.co/TheBloke/Llama-2-70B-chat-GPTQ\n",
    "# To use a different branch, change revision\n",
    "# For example: revision=\"gptq-4bit-32g-actorder_True\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype = torch.float16,\n",
    "    device_map = \"auto\",\n",
    "    revision=\"gptq-4bit-32g-actorder_True\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token_id = (\n",
    "    0\n",
    ")\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "CF3DNOWLpLak"
   },
   "outputs": [],
   "source": [
    "pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "BGBJUUKIpwwr"
   },
   "outputs": [],
   "source": [
    "def generation_function(prompt: str):\n",
    "  features = pipeline(prompt)[0][\"generated_text\"][len(prompt):]\n",
    "  features = features.split(\"\\n-\")\n",
    "  features = [feat.replace(\"\\n\", \"\") for feat in features]\n",
    "  features = [feat.strip()for feat in features]\n",
    "  features = [feat for feat in features if len(feat)>0]\n",
    "  features = set(features)\n",
    "  return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "4xAD06g7pwhV"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [03:09<00:00,  9.45s/it]\n"
     ]
    }
   ],
   "source": [
    "generated_answers = {}\n",
    "output_file = \"data/concepts_llama.json\"\n",
    "# XXX: all_concepts or all_classes?\n",
    "save_classes = \"data/all_concepts.txt\"\n",
    "with open(save_classes, \"r\") as f:\n",
    "  classes = f.read().lower().split(\"\\n\")\n",
    "\n",
    "# NOTE: change as needed to all classes\n",
    "for label in tqdm(classes[:20]):\n",
    "  generated_answers[label] = {}\n",
    "\n",
    "  prompt1 = f\"List the most important features for recognizing something as a {label}? Write them one by one.\"\n",
    "  generated1 = generation_function(prompt1)\n",
    "  generated_answers[label][\"A1\"] = list(generated1)\n",
    "\n",
    "  prompt2 = f\"List the things most commonly seen around a {label}. Write them one by one.\"\n",
    "  generated2 = generation_function(prompt2)\n",
    "  generated_answers[label][\"A2\"] = list(generated2)\n",
    "\n",
    "  prompt3 = f\"Give a generalization for the word {label}\"\n",
    "  generated3 = generation_function(prompt3)\n",
    "  generated_answers[label][\"A3\"] = list(generated3)\n",
    "\n",
    "\n",
    "with open(output_file, \"w\") as json_file:\n",
    "    json.dump(generated_answers, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njmCcEPVqgZs"
   },
   "source": [
    "TheBloke/llama-2-70b-Guanaco-QLoRA-fp16\n",
    "\n",
    "**System:**\n",
    "{prompt}\n",
    "\n",
    "**Assistant:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "IE9r4bVBqk9V",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4ZwOz3HpurH"
   },
   "source": [
    "TheBloke/Llama-2-70B-GGML\n",
    "\n",
    "\n",
    "**System:**\n",
    "{system_message}\n",
    "\n",
    "**User:**\n",
    "{prompt}\n",
    "\n",
    "**Assistant:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "OhMFDJOwq4i-",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNiX6Y7drFB8dzS5pWr05tY",
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "GEMBA-HjTQyIiq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
