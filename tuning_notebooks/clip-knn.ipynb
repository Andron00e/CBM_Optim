{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06c8dc3a-8a15-4754-aea4-1c7564a861b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-30 00:47:59.970701: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-30 00:47:59.970787: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-30 00:47:59.970810: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-30 00:47:59.978534: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-30 00:48:00.989526: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import random\n",
    "import configs\n",
    "import numpy as np\n",
    "import transformers\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn.functional as F\n",
    "from training_utils import BottleneckTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a55e9740-824a-4c83-87f2-ca339a7825ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "configs.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48cbf1a2-ceaf-451e-a9dc-f99ec51c370a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8 GPU(s) available.\n",
      "We will use the GPU: NVIDIA A100-SXM4-80GB\n"
     ]
    }
   ],
   "source": [
    "device = configs.set_device(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e162dd9-7c32-4b64-9db6-c9b7af2d9b02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "hf_dataset = load_dataset(\"Andron00e/CIFAR10-custom\")\n",
    "dataset = hf_dataset[\"train\"].train_test_split(test_size=0.2)\n",
    "val_test = dataset[\"test\"].train_test_split(test_size=0.5)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": dataset[\"train\"],\n",
    "    \"validation\": val_test[\"train\"],\n",
    "    \"test\": val_test[\"test\"],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "722734a5-4b65-4744-a83e-b93e102099db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image_file_path', 'image', 'labels'],\n",
       "        num_rows: 48000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['image_file_path', 'image', 'labels'],\n",
       "        num_rows: 6000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image_file_path', 'image', 'labels'],\n",
       "        num_rows: 6000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "813a3ae0-34f8-4c58-af24-56f2e364d281",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"openai/clip-vit-base-patch32\"\n",
    "clip = transformers.CLIPModel.from_pretrained(model_name)\n",
    "processor = transformers.CLIPProcessor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fda6bd0-e671-4193-8296-307785131783",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_prefixes(strings):\n",
    "    prefixes = ['a', 'an', 'the']\n",
    "    result = []\n",
    "\n",
    "    for string in strings:\n",
    "        words = string.split()\n",
    "        if words[0].lower() in prefixes:\n",
    "            result.append(' '.join(words[1:]))\n",
    "        else:\n",
    "            result.append(string)\n",
    "\n",
    "    return result\n",
    "\n",
    "with open(\"conceptnet_cifar10_filtered_new.txt\", \"r\") as f:\n",
    "    concepts = f.read().lower().split(\"\\n\")\n",
    "    concepts = remove_prefixes(concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffaffe5c-c55d-41cc-8a51-121653a0ed2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def contrastive_loss(logits, dim):\n",
    "    neg_ce = torch.diag(F.log_softmax(logits, dim=dim))\n",
    "    return -neg_ce.mean()\n",
    "    \n",
    "def clip_loss(similarity: torch.Tensor) -> torch.Tensor:\n",
    "    caption_loss = contrastive_loss(similarity, dim=0)\n",
    "    image_loss = contrastive_loss(similarity, dim=1)\n",
    "    return (caption_loss + image_loss) / 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26895b9f-0ee8-468d-a6f6-a6724d3468c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "classes = {0: \"airplane\", 1: \"automobile\", 2: \"bird\", 3: \"cat\", 4: \"deer\", 5: \"dog\", 6: \"frog\", 7: \"horse\", 8: \"ship\", 9: \"truck\"}\n",
    "\n",
    "def label_to_word(label: int) -> str:\n",
    "    return classes[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e94ede67-5e4f-4d56-9a37-a2f4fcafc1b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 \n",
      "\n",
      "10000 \n",
      "\n",
      "60000 \n",
      "\n",
      "Dataset size: 60000 \n",
      "\n",
      "Train set: 50000 \n",
      "\n",
      "Validation set: 3000 \n",
      "\n",
      "Test set: 7000\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
    "\n",
    "\n",
    "image_paths = []\n",
    "labels = []\n",
    "\n",
    "save_dir = 'cifar10_images'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "for i in range(len(train_images)):\n",
    "    image_path = os.path.join(save_dir, f\"train_image_{i}.jpg\")\n",
    "    tf.keras.preprocessing.image.save_img(image_path, train_images[i])\n",
    "    image_paths.append(image_path)\n",
    "    labels.append(train_labels[i][0])\n",
    "\n",
    "\n",
    "for i in range(len(test_images)):\n",
    "    image_path = os.path.join(save_dir, f\"test_image_{i}.jpg\")\n",
    "    tf.keras.preprocessing.image.save_img(image_path, test_images[i])\n",
    "    image_paths.append(image_path)\n",
    "    labels.append(test_labels[i][0])\n",
    "\n",
    "print(len(train_images), \"\\n\")\n",
    "print(len(test_images), \"\\n\")\n",
    "print(len(image_paths), \"\\n\")\n",
    "\n",
    "class CLIPDataset():\n",
    "    def __init__(self, list_image_path, list_txt):\n",
    "        self.image_path = list_image_path\n",
    "        self.title  = list_txt\n",
    "        #self.title_text = [classes[l] for l in list_txt]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.title)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_path[idx])\n",
    "        title = self.title[idx]\n",
    "        #title_text = self.title_text[idx]\n",
    "        return image, title, #title_text\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'image': [x[0] for x in batch],\n",
    "        'title': [x[1] for x in batch],\n",
    "        #'title-text': [x[2] for x in batch]\n",
    "    }\n",
    "\n",
    "dataset = CLIPDataset(list_image_path=image_paths, list_txt=labels) # but it can be with <<titles>> to get textual annotations for class labels\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [50000, 3000, 7000])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn, pin_memory=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn, pin_memory=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn, pin_memory=True)\n",
    "\n",
    "assert len(dataset) == len(image_paths)\n",
    "print(\"Dataset size: {}\".format(len(dataset)), \"\\n\")\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [50000, 3000, 7000])\n",
    "print(\"Train set: {}\".format(len(train_dataset)), \"\\n\")\n",
    "print(\"Validation set: {}\".format(len(val_dataset)), \"\\n\")\n",
    "print(\"Test set: {}\".format(len(test_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a68ef69b-fc93-4eb6-84a1-db2afec7049d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import CLIPModel, CLIPProcessor, AutoTokenizer\n",
    "\n",
    "def preprocess_loader(loader):\n",
    "    preprocessed_batches = []\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    for batch in tqdm(loader):\n",
    "        preprocessed_batch = preprocess_batch(batch, processor)\n",
    "        preprocessed_batches.append(preprocessed_batch)\n",
    "    return preprocessed_batches\n",
    "\n",
    "def preprocess_batch(batch, processor):\n",
    "    return processor(text=list(classes.values()), images=batch['image'], return_tensors=\"pt\", padding=True), batch['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ca11a30-d48f-4ef0-8246-203383b75875",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bfd44b467a34d3990cd4c51a710052d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54e84da166ba44da8a1da3fc621ade14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43dcf123a39c4b5cb15c923b833f5522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/219 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loader_preprocessed = preprocess_loader(train_loader)\n",
    "val_loader_preprocessed = preprocess_loader(val_loader)\n",
    "test_loader_preprocessed = preprocess_loader(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d590771-93aa-4158-a1fd-e9d2a8a6b8ee",
   "metadata": {},
   "source": [
    "classes text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e501ebea-0579-4d58-9651-4ca1abbd2c52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "classes_inputs = processor.tokenizer(list(classes.values()), padding=True, return_tensors=\"pt\")\n",
    "classes_features = clip.get_text_features(**classes_inputs)\n",
    "classes_features /= classes_features.norm(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5023813-f783-4c42-86d4-352bf0d1edd6",
   "metadata": {},
   "source": [
    "concepts text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2651ff2-a55f-4c8e-94b7-e6988eb57d2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "concepts_inputs = processor.tokenizer(concepts, padding=True, return_tensors=\"pt\")\n",
    "concepts_features = clip.get_text_features(**concepts_inputs)\n",
    "concepts_features /= concepts_features.norm(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478f7920-d0a2-4797-a861-f2884596c6ff",
   "metadata": {},
   "source": [
    "dict of concepts encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56126319-f1b0-4a20-a2da-17d3d3ea46b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "concept_encodings = {}\n",
    "for concept, concept_vector in zip(concepts, concepts_features):\n",
    "    concept_encodings[concept]  = concept_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0c93af-6120-42e4-88c2-2517f0129c36",
   "metadata": {},
   "source": [
    "dict of classes encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "722fab46-8d8e-4190-9dec-6c5c3eaa3b82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "classes_encodings = {}\n",
    "for class_name, class_vector in zip(classes.values(), classes_features):\n",
    "    concept_encodings[class_name]  = class_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32b6517c-d615-4a1a-a220-234424978c8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def aggregate_similarity(similarity_matrix_chunk, aggregation_method='mean'):\n",
    "    if aggregation_method == 'max': return similarity_matrix_chunk.max(dim=1)[0]\n",
    "    elif aggregation_method == 'sum': return similarity_matrix_chunk.sum(dim=1)\n",
    "    elif aggregation_method == 'mean': return similarity_matrix_chunk.mean(dim=1)\n",
    "    else: raise ValueError(\"Unknown aggregate_similarity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "8c0b92ab-fb67-46d9-8033-7d4984601d0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b93c2b19b9e648b6a8a02172ac2fe9e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([20.2827, 20.0167, 20.6020, 19.9099, 20.7902, 21.9529, 21.8211, 20.2437,\n",
      "        21.4387, 21.5623, 20.5779, 21.0109, 20.2587, 21.6531, 21.0925, 22.0862,\n",
      "        19.8738, 21.6007, 21.1770, 21.8955, 20.9331, 21.1743, 20.9983, 21.2279,\n",
      "        21.7373, 21.4769, 21.1718, 21.0591, 20.6278, 20.7612, 21.3855, 21.0181],\n",
      "       grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "for batch_num, batch in enumerate(tqdm(train_loader)):\n",
    "    images, labels, titles = batch['image'], batch['title'], batch['title-text']\n",
    "    classes_inputs = processor(text = list(classes.values()), images=images, padding=True, return_tensors=\"pt\")\n",
    "    concept_inputs = processor(text = concepts, images=images, padding=True, return_tensors=\"pt\")\n",
    "    classes_out = clip(**classes_inputs)\n",
    "    concept_out = clip(**concept_inputs)\n",
    "    #print(out.logits_per_image)\n",
    "    #print(classes_out.logits_per_image.softmax(dim=1), \"\\n\")\n",
    "    #print(classes_out.logits_per_image.softmax(dim=1)[0].argmax())\n",
    "    #print(labels[0], \"\\n\")\n",
    "    #print(concept_out.logits_per_image.softmax(dim=1)[0].argmax())\n",
    "    print(aggregate_similarity(concept_out.logits_per_image))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1beb47df-93d1-4099-b4a1-b97a8c4f1941",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23e547b027194c42985969baa43fe645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_preds = []\n",
    "all_labels = []\n",
    "clip.to(device)\n",
    "for batch_num, batch in enumerate(tqdm(train_loader_preprocessed)):\n",
    "    inputs, labels = batch\n",
    "    inputs = inputs.to(device)\n",
    "    classes_out = clip(**inputs).logits_per_image\n",
    "    probs = classes_out.softmax(dim=1)\n",
    "    preds = torch.argmax(probs, dim=1)\n",
    "    all_preds.extend(list(preds.cpu().numpy()))\n",
    "    all_labels.extend(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a729bf-d41b-424a-89e8-a6eeb3ffb8b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "metric = datasets.load_metric(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895e53ef-22da-4a9a-85a3-109d2ef50ec4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "accuracy = metric.compute(predictions=all_preds, references=all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87154dc0-a0f3-41dd-b697-8397179e242d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a250e6d2-a6e3-42d9-94e4-df2448e1335b",
   "metadata": {},
   "source": [
    "## cummulative similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201520a0-a5bb-463e-b040-5bc052e4073b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cummulative_similarity()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "andronserv",
   "language": "python",
   "name": "andronserv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
